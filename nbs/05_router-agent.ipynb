{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "_ = load_dotenv(\"../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "local_llm = \"qwen2.5:7b\"\n",
    "# llm = ChatOllama(model=local_llm, temperature=0)\n",
    "# llm = ChatOllama(model=\"qwen2.5:7b\", temperature=0)\n",
    "llm = ChatAnthropic(model=\"claude-3-5-sonnet-20241022\", temperature=0)\n",
    "llm_json_mode = ChatOllama(model=local_llm, temperature=0, format=\"json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### VectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "db = Chroma(persist_directory=\"../data/chroma_db\", embedding_function=embedder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# Prompt\n",
    "router_instructions = \"\"\"You are an expert at routing a user question to a vectorstore or tool call.\n",
    "The vectorstore contains details about datasets from World Resource Institute(WRI).\n",
    "Use the vectorstore for questions on topics related to searching datasets. \n",
    "For specific question on forest fires use the tool call.\n",
    "Return JSON with single key, route, that is 'vectorstore' or 'glad-tool' depending on the question.\"\"\"\n",
    "\n",
    "queries = [\n",
    "    \"I am interested in biodiversity conservation in Argentina\",\n",
    "    \"I would like to explore helping with forest loss in Amazon\",\n",
    "    \"show datasets related to mangrooves\",\n",
    "    \"find forest fires in milan for the year 2022\",\n",
    "    \"show stats on forest fires over Ihorombe for 2021\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tests\n",
    "for query in queries:\n",
    "    response = llm_json_mode.invoke(\n",
    "        [SystemMessage(content=router_instructions)] + [HumanMessage(content=query)]\n",
    "    )\n",
    "    response = json.loads(response.content)\n",
    "    print(query, \" ---> \", response[\"route\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_prompt = \"\"\"You are a World Resources Institute (WRI) assistant specializing in dataset recommendations.\n",
    "\n",
    "Instructions:\n",
    "1. Use the following context to inform your response:\n",
    "{context}\n",
    "\n",
    "2. User Question:\n",
    "{question}\n",
    "\n",
    "3. Response Format:\n",
    "   - Only use information from the provided context\n",
    "   - For each recommended dataset:\n",
    "     - Dataset URL\n",
    "     - Two-line explanation of why this dataset is relevant to the user's problem\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"I am interested in biodiversity conservation in Argentina\"\n",
    "docs = retriever.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_context(docs):\n",
    "    fmt_docs = []\n",
    "    for doc in docs:\n",
    "        url = (\n",
    "            f\"https://data-api.globalforestwatch.org/dataset/{doc.metadata['dataset']}\"\n",
    "        )\n",
    "        content = \"URL: \" + url + \"\\n\" + doc.page_content\n",
    "        fmt_docs.append(content)\n",
    "    return \"\\n\\n\".join(fmt_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_txt = make_context(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_prompt_fmt = rag_prompt.format(context=docs_txt, question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation = llm.invoke([HumanMessage(content=rag_prompt_fmt)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generation.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing_extensions import TypedDict\n",
    "from typing import List, Annotated\n",
    "from IPython.display import Image, display, Markdown\n",
    "from langgraph.graph import START, MessagesState, StateGraph, END, add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langchain_core.messages import AnyMessage, ToolMessage\n",
    "\n",
    "from src.tools.glad.weekly_alerts_tool import glad_weekly_alerts_tool\n",
    "from src.tools.location.tool import location_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [location_tool, glad_weekly_alerts_tool]\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "    question: str  # User question\n",
    "    generation: str  # LLM generation\n",
    "    answers: int  # Number of answers generated\n",
    "    loop_step: Annotated[int, operator.add]\n",
    "    documents: List[str]  # List of retrieved documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state):\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\"documents\": documents}\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    loop_step = state.get(\"loop_step\", 0)\n",
    "\n",
    "    # RAG generation\n",
    "    docs_txt = make_context(documents)\n",
    "    rag_prompt_fmt = rag_prompt.format(context=docs_txt, question=question)\n",
    "    generation = llm.invoke([HumanMessage(content=rag_prompt_fmt)])\n",
    "    return {\"generation\": generation, \"loop_step\": loop_step + 1}\n",
    "\n",
    "\n",
    "def assistant(state):\n",
    "    sys_msg = SystemMessage(\n",
    "        content=\"\"\"You are a helpful assistant tasked with answering the user queries for WRI data API.\n",
    "        Use the `location-tool` to get iso, adm1 & adm2 of any region or place.\n",
    "        Use the `glad-weekly-alerts-tool` to get forest fire information for a particular year. Think through the solution step-by-step first and then execute.\n",
    "        \n",
    "        For eg: If the query is \"Find forest fires in Milan for the year 2024\"\n",
    "        Steps\n",
    "        1. Use the `location_tool` to get iso, adm1, adm2 for place `Milan` by passing `query=Milan`\n",
    "        2. Pass iso, adm1, adm2 along with year `2024` as args to `glad-weekly-alerts-tool` to get information about forest fire alerts.\n",
    "        \"\"\"\n",
    "    )\n",
    "    if not state[\"messages\"]:\n",
    "        state[\"messages\"] = [HumanMessage(state[\"question\"])]\n",
    "    return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])]}\n",
    "\n",
    "\n",
    "tool_node = ToolNode(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools_by_name = {tool.name: tool for tool in tools}\n",
    "# def tool_node(state: dict):\n",
    "#     result = []\n",
    "#     for tool_call in state[\"messages\"][-1].tool_calls:\n",
    "#         tool = tools_by_name[tool_call[\"name\"]]\n",
    "#         print(tool)\n",
    "#         observation = tool.invoke(tool_call[\"args\"])\n",
    "#         result.append(ToolMessage(content=observation, tool_call_id=tool_call[\"id\"]))\n",
    "#     return {\"messages\": result}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def router(state):\n",
    "    print(\"---ROUTER---\")\n",
    "    response = llm_json_mode.invoke(\n",
    "        [SystemMessage(content=router_instructions)]\n",
    "        + [HumanMessage(content=state[\"question\"])]\n",
    "    )\n",
    "    route = json.loads(response.content)[\"route\"]\n",
    "    if route == \"vectorstore\":\n",
    "        print(\"---ROUTING-TO-RAG---\")\n",
    "        return \"retrieve\"\n",
    "    elif route == \"glad-tool\":\n",
    "        print(\"---ROUTING-TO-TOOLS---\")\n",
    "        return \"assistant\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_tool(state):\n",
    "    # set_trace()\n",
    "    last_msg = state[\"messages\"][-1]\n",
    "    if not last_msg.tool_calls:\n",
    "        return \"__end__\"\n",
    "    return \"tools\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "wf = StateGraph(GraphState)\n",
    "\n",
    "wf.add_node(\"retrieve\", retrieve)\n",
    "wf.add_node(\"generate\", generate)\n",
    "wf.add_node(\"assistant\", assistant)\n",
    "wf.add_node(\"tools\", tool_node)\n",
    "\n",
    "wf.set_conditional_entry_point(\n",
    "    router, {\"retrieve\": \"retrieve\", \"assistant\": \"assistant\"}\n",
    ")\n",
    "wf.add_edge(\"retrieve\", \"generate\")\n",
    "wf.add_edge(\"generate\", END)\n",
    "wf.add_conditional_edges(\"assistant\", tools_condition)\n",
    "wf.add_edge(\"tools\", \"assistant\")\n",
    "wf.add_edge(\"assistant\", END)\n",
    "\n",
    "graph = wf.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(graph.get_graph(xray=False).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = graph.invoke({\"question\": \"show stats on forest fires over Ihorombe for 2021\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "for msg in result[\"messages\"]:\n",
    "    msg.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = graph.invoke({\"question\": \"I am interested in tree cover loss over amazon\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(result[\"generation\"].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
