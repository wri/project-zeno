{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring agent data generation\n",
    "\n",
    "This notebooks documents how the KBA data was generated for the monitoring prototype.\n",
    "\n",
    "The concept is that all data is static. It is pre-generated for all the Key\n",
    "Biodiversity Areas (KBA). The statistics are then compiled into unified datasets that\n",
    "will be accessed by the monitoring agent.\n",
    "\n",
    "We have used the following files\n",
    "\n",
    "- KBA Shapefile containing the geometries and simple attributes like names\n",
    "- Metadata for each KBA with detailed descriptions of each area\n",
    "- Static data from Global Forest Watch, compiled without time dimenison. Most data is changes between 2001 and 2023.\n",
    "- Time series data from Grasslands dataset, in yearly increments\n",
    "- Time series data for tree cover loss and associated carbon emissions\n",
    "\n",
    "### Static data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "\n",
    "DATADIR = \"zeno/agents/monitoring/kba_data_preparation\"\n",
    "\n",
    "# Read source data and unify sitecode column\n",
    "kba = gpd.read_file(\n",
    "    f\"{$DATADIR}/kba_global_2024_semptember.gpkg\"\n",
    ")\n",
    "kba.rename(columns={\"SitRecID\": \"sitecode\"}, inplace=True)\n",
    "\n",
    "# KBA static data\n",
    "df_kba_static = pd.read_csv(\n",
    "    f\"{$DATADIR}/KBA info scrape - static data.csv\"\n",
    ")\n",
    "df_kba_static.drop_duplicates(subset=\"sitecode\", inplace=True)\n",
    "\n",
    "df_kba_tcl = pd.read_csv(\n",
    "    f\"{$DATADIR}/KBA info scrape - tree cover loss drivers data.csv\"\n",
    ")\n",
    "df_kba_tcl.rename(columns={\"SitRecID\": \"sitecode\"}, inplace=True)\n",
    "\n",
    "# KBA detailed descriptions\n",
    "df_kba_meta = pd.read_csv(\n",
    "    f\"{$DATADIR}/KBA info scrape - scraped kba metadata.csv\"\n",
    ")\n",
    "\n",
    "# Merge different source file into single dataframe\n",
    "merged = kba.merge(df_kba_static, on=\"sitecode\", suffixes=(\"\", \"_dup\"), how=\"left\")\n",
    "merged = merged.loc[:, ~merged.columns.str.endswith(\"_dup\")]\n",
    "\n",
    "merged = merged.merge(df_kba_tcl, on=\"sitecode\", suffixes=(\"\", \"_dup\"), how=\"left\")\n",
    "merged = merged.loc[:, ~merged.columns.str.endswith(\"_dup\")]\n",
    "\n",
    "merged = merged.merge(df_kba_meta, on=\"sitecode\", suffixes=(\"\", \"_dup\"), how=\"left\")\n",
    "merged = merged.loc[:, ~merged.columns.str.endswith(\"_dup\")]\n",
    "\n",
    "# Check for consistency\n",
    "print(f\"Unique sitecodes in kba: {len(kba['sitecode'].unique())}, {kba.shape[0]}\")\n",
    "print(\n",
    "    f\"unique sitecodes in df_kba_meta: {len(df_kba_meta['sitecode'].unique())}, {df_kba_meta.shape[0]}\"\n",
    ")\n",
    "print(\n",
    "    f\"unique sitecodes in df_kba_static_data: {len(df_kba_static['sitecode'].unique())}, {df_kba_static.shape[0]}\"\n",
    ")\n",
    "print(\n",
    "    f\"unique sitecodes in df_kba_tcl: {len(df_kba_tcl['sitecode'].unique())}, {df_kba_tcl.shape[0]}\"\n",
    ")\n",
    "print(\n",
    "    f\"unique sitecodes in merged: {len(merged['sitecode'].unique())}, {merged.shape[0]}\"\n",
    ")\n",
    "\n",
    "DESCRIBED_FIELDS = [\n",
    "    \"area__ha\",\n",
    "    \"umd_tree_cover_extent_2000__ha\",\n",
    "    \"umd_tree_cover_gain__ha\",\n",
    "    \"gfw_forest_carbon_gross_removals_aboveground_2001_2023__Mg_CO2\",\n",
    "    \"gfw_forest_carbon_gross_removals_belowground_2001_2023__Mg_CO2\",\n",
    "    \"gfw_forest_carbon_gross_removals_2001_2023__Mg_CO2\",\n",
    "    \"gfw_forest_carbon_gross_emissions_all_gases_2001_2023__Mg_CO2e\",\n",
    "    \"gfw_forest_carbon_net_flux_2001_2023__Mg_CO2e\",\n",
    "    \"gfw_aboveground_carbon_stock_2000__Mg_C\",\n",
    "    \"gfw_belowground_carbon_stock_2000__Mg_C\",\n",
    "    \"gfw_soil_carbon_stock_2000__Mg_C\",\n",
    "    \"whrc_aboveground_biomass_stock_2000__Mg\",\n",
    "    \"avg_whrc_aboveground_biomass_density_2000__Mg_ha-1\",\n",
    "    \"permAg_tcl_2001-2023\",\n",
    "    \"hardCommodities_tcl_2001-2023\",\n",
    "    \"shifting_tcl_2001-2023\",\n",
    "    \"forestMgmt_tcl_2001-2023\",\n",
    "    \"wildfire_tcl_2001-2023\",\n",
    "    \"settlements_tcl_2001-2023\",\n",
    "    \"natural_tcl_2001-2023\",\n",
    "    \"additionalBiodiversityValues\",\n",
    "    \"calculatedProtectedArea\",\n",
    "    \"country\",\n",
    "    \"deliniationRationale\",\n",
    "    \"ecosystems\",\n",
    "    \"elevation(M)\",\n",
    "    \"globalKbaCriteria\",\n",
    "    \"habitatDescription\",\n",
    "    \"howIsTheSiteManaged\",\n",
    "    \"indigenousGroups\",\n",
    "    \"irreplaceabilityAssessmentApproved\",\n",
    "    \"kbaClassification\",\n",
    "    \"landUseRegimesAtSite\",\n",
    "    \"latitude\",\n",
    "    \"longitude\",\n",
    "    \"rationaleForSiteInformation\",\n",
    "    \"references\",\n",
    "    \"regions\",\n",
    "    \"siteAreaCalculated\",\n",
    "    \"sitecode\",\n",
    "    \"siteDescription\",\n",
    "    \"siteName\",\n",
    "    \"siteNameNational\",\n",
    "    \"threatsDescription\",\n",
    "    \"updatedAt\",\n",
    "    \"yearOfAssessment\",\n",
    "    \"geometry\",\n",
    "]\n",
    "\n",
    "for bla in DESCRIBED_FIELDS:\n",
    "    if bla not in merged.columns:\n",
    "        print(f\"Missing field: {bla}\")\n",
    "\n",
    "\n",
    "merged = merged[DESCRIBED_FIELDS]\n",
    "\n",
    "print(\"Should be same\", np.unique(merged[\"sitecode\"]).shape, merged.shape[0])\n",
    "\n",
    "merged.to_file(f\"{$DATADIR}/kba_merged.gpkg\", driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time series data preparation\n",
    "\n",
    "The dime series data is prepared in a \"melted\" format. That means that the\n",
    "table will have a multi-index where the combination of `sitecode` and `year`\n",
    "is unique. For each of thes combinations, we compile different values.\n",
    "\n",
    "The result outputs two tables, one for grasslands, and one for tree cover loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "gpp_data = pd.read_csv(f\"${DATADIR}/GPW_grass_GPP.csv\")\n",
    "area_data = pd.read_csv(f\"${DATADIR}/GPW_grass_class_area.csv\")\n",
    "kba_data = pd.read_csv(f\"${DATADIR}/KBA info scrape - annual data.csv\")\n",
    "kba_data = kba_data[\n",
    "    [\n",
    "        \"sitecode\",\n",
    "        \"year\",\n",
    "        \"gfw_forest_carbon_gross_emissions_all_gases\",\n",
    "        \"umd_tree_cover_loss\",\n",
    "    ]\n",
    "]\n",
    "kba_data.set_index([\"sitecode\", \"year\"], inplace=True)\n",
    "\n",
    "result = None\n",
    "for key in [\"GPP\", \"cultivated\", \"nsn\"]:\n",
    "    print(key)\n",
    "    data = gpp_data if key == \"GPP\" else area_data\n",
    "\n",
    "    colnames = [dat for dat in data.columns if f\"_{key}_\" in dat]\n",
    "    print(colnames)\n",
    "    years = [dat.split(\"_\")[0] for dat in colnames]\n",
    "\n",
    "    dfd = {year: data[colname] for year, colname in zip(years, colnames)}\n",
    "    dfd[\"sitecode\"] = data[\"SitRecID\"]\n",
    "\n",
    "    df = pd.DataFrame(dfd)\n",
    "\n",
    "    df = df.melt(id_vars=[\"sitecode\"], var_name=\"year\", value_name=key)\n",
    "\n",
    "    df[\"year\"] = df[\"year\"].astype(int)\n",
    "\n",
    "    df.columns = [\"sitecode\", \"year\", key]\n",
    "\n",
    "    df.set_index([\"sitecode\", \"year\"], inplace=True)\n",
    "\n",
    "    if result is None:\n",
    "        result = df\n",
    "    else:\n",
    "        result = result.merge(df, left_index=True, right_index=True, how=\"outer\")\n",
    "\n",
    "final = result.merge(kba_data, left_index=True, right_index=True, how=\"outer\")\n",
    "\n",
    "final.reset_index(inplace=True)\n",
    "\n",
    "# Check consistency\n",
    "print(final[final[\"sitecode\"] == 8010])\n",
    "\n",
    "final.to_parquet(f\"${DATADIR}/kba_timeseries_data.parquet\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
